{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Shortcut Learning Mitigation with Task Arithmetic (Waterbirds) \u2014 Kaggle Notebook\n\nThis notebook is a **fully runnable, end-to-end pipeline** that demonstrates:\n\n1. **Shortcut learning** on the *Waterbirds* benchmark (spurious correlation between bird type and background).\n2. **Checkpoint / snapshot logging** during training.\n3. **Task vectors**:  \\(v_T = w_{ft} - w_{pre}\\)\n4. **Task arithmetic edits**: add / scale / negate task vectors in weight space.\n5. **Trajectory analysis**: PCA of weight trajectories + alignment with the final task vector.\n6. **Evaluation**: overall accuracy, per-group accuracy, and **worst-group accuracy**.\n7. **Plots + saved artifacts** (PNG/JSON/CSV) written to `outputs/`.\n\n> **Kaggle note**: This notebook tries to download Waterbirds via the `wilds` library.  \n> If downloads are blocked, you can still run by adding a Kaggle dataset that contains Waterbirds/CUB files and pointing `DATA_ROOT` to it (instructions are included below)."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0) Install / imports"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import sys, subprocess, pkgutil\n\ndef pip_install(pkg: str):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"-q\", \"install\", pkg])\n\nmissing = []\nfor pkg in [\"wilds\", \"scikit-learn\"]:\n    if pkgutil.find_loader(pkg.replace(\"-\", \"_\")) is None and pkgutil.find_loader(pkg) is None:\n        missing.append(pkg)\n\nif missing:\n    print(\"Installing:\", missing)\n    for m in missing:\n        pip_install(m)\nelse:\n    print(\"All required packages already installed.\")"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import os\nimport re\nimport math\nimport json\nimport csv\nimport random\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple, Optional\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\n\nfrom torchvision import transforms, models\n\nfrom wilds import get_dataset\n\nprint(\"torch:\", torch.__version__)\nprint(\"cuda available:\", torch.cuda.is_available())\nprint(\"mps available:\", hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Background: what we're doing\n\n### Shortcut learning (spurious correlations)\nA *shortcut* is a feature that's easy for the model to use on the training distribution, but **does not generalize** when the correlation breaks.\n\nIn Waterbirds:\n- **Core label**: bird type (waterbird vs landbird).\n- **Spurious attribute**: background (water vs land).\n\n### Task vectors + task arithmetic\nLet:\n- \\(w_{pre}\\): pretrained weights\n- \\(w_{ft}\\): weights after fine-tuning on task \\(T\\)\n\nDefine task vector:\n\\[\nv_T = w_{ft} - w_{pre}\n\\]\n\nTask arithmetic edits:\n\\[\nw_{new} = w_{pre} + \\alpha v_T\n\\]\n\n- \\(\\alpha = 1\\): approx. the fine-tuned model  \n- \\(\\alpha = 0.5\\): \"halfway\" fine-tune  \n- \\(\\alpha = -1\\): **negate** (often \u201cforget\u201d the task)  \n\n### What counts as success here?\nWe track:\n- **overall accuracy**\n- **group accuracy** (group = (label, background))\n- **worst-group accuracy**"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Config + reproducibility"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "SEED = 42\n\nMAX_STEPS = 300          # increase for better performance (e.g., 1500-3000) if you have time\nBATCH_SIZE = 64\nLR = 1e-4\nWEIGHT_DECAY = 0.0\n\nSNAPSHOT_EVERY = 50\nEVAL_EVERY = 50\nNUM_WORKERS = 2\n\nOUTPUT_ROOT = Path(\"outputs/waterbirds\")\nLOG_DIR = OUTPUT_ROOT / \"logs\"\nRES_DIR = OUTPUT_ROOT / \"results\"\nSNAP_DIR = LOG_DIR / \"snapshots\"\nfor p in [LOG_DIR, RES_DIR, SNAP_DIR]:\n    p.mkdir(parents=True, exist_ok=True)\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(SEED)\n\nif torch.cuda.is_available():\n    DEVICE = torch.device(\"cuda\")\nelif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n    DEVICE = torch.device(\"mps\")\nelse:\n    DEVICE = torch.device(\"cpu\")\n\nprint(\"DEVICE:\", DEVICE)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Load Waterbirds (via WILDS)\n\nWe use the `wilds` package:\n- `get_dataset(\"waterbirds\")`\n- `get_subset(\"train\")`, `get_subset(\"val\")`, `get_subset(\"test\")`\n\nIf the download fails on Kaggle (no internet), you can:\n1. Add a Kaggle dataset that contains Waterbirds files.\n2. Set `DATA_ROOT` to that mounted path (usually `/kaggle/input/<dataset-name>/...`).\n\nThe code below auto-detects the background field from `metadata_fields`."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "DATA_ROOT = None  # set to a path if you mounted a Kaggle dataset manually\n\nIMG_SIZE = 224\ntransform = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.CenterCrop(IMG_SIZE),\n    transforms.ToTensor(),\n])\n\ndef find_background_index(metadata_fields: List[str]) -> int:\n    candidates = [\"place\", \"background\", \"env\", \"environment\"]\n    lower = [f.lower() for f in metadata_fields]\n    for cand in candidates:\n        for i, f in enumerate(lower):\n            if cand in f:\n                return i\n    return 0\n\nclass WaterbirdsWILDSWrapper(Dataset):\n    def __init__(self, subset, transform, bg_index: int):\n        self.subset = subset\n        self.transform = transform\n        self.bg_index = bg_index\n\n    def __len__(self):\n        return len(self.subset)\n\n    def __getitem__(self, idx: int):\n        x, y, meta = self.subset[idx]\n        if self.transform is not None:\n            x = self.transform(x)\n        y = y.long()\n        spurious = int(meta[self.bg_index].item())\n        group = int(y.item()) * 2 + spurious\n        return x, {\"y\": y, \"spurious\": torch.tensor(spurious), \"group\": torch.tensor(group)}\n\ndataset = get_dataset(dataset=\"waterbirds\", root_dir=DATA_ROOT)\nbg_idx = find_background_index(dataset.metadata_fields)\nprint(\"metadata_fields:\", dataset.metadata_fields)\nprint(\"Using background field index:\", bg_idx, \"->\", dataset.metadata_fields[bg_idx])\n\ntrain_ds = WaterbirdsWILDSWrapper(dataset.get_subset(\"train\"), transform, bg_idx)\nval_ds   = WaterbirdsWILDSWrapper(dataset.get_subset(\"val\"), transform, bg_idx)\ntest_ds  = WaterbirdsWILDSWrapper(dataset.get_subset(\"test\"), transform, bg_idx)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\nval_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\ntest_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n\nxb, yb = next(iter(train_loader))\nprint(\"batch x:\", xb.shape, xb.dtype)\nprint(\"y:\", yb[\"y\"][:5].tolist(), \"spurious:\", yb[\"spurious\"][:5].tolist(), \"group:\", yb[\"group\"][:5].tolist())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Metrics: overall accuracy + group accuracy + worst-group accuracy"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef evaluate_grouped(model: nn.Module, loader: DataLoader, device: torch.device) -> Dict[str, Any]:\n    model.eval()\n    ce = nn.CrossEntropyLoss()\n\n    total = 0\n    correct = 0\n    loss_sum = 0.0\n\n    group_correct: Dict[int, int] = {}\n    group_total: Dict[int, int] = {}\n\n    for x, ydict in loader:\n        x = x.to(device)\n        y = ydict[\"y\"].to(device)\n\n        logits = model(x)\n        loss = ce(logits, y)\n        pred = logits.argmax(dim=-1)\n\n        correct += (pred == y).sum().item()\n        total += y.numel()\n        loss_sum += loss.item() * y.numel()\n\n        groups = ydict[\"group\"].cpu().numpy().astype(int)\n        matches = (pred == y).cpu().numpy().astype(int)\n        for g, m in zip(groups, matches):\n            group_total[g] = group_total.get(g, 0) + 1\n            group_correct[g] = group_correct.get(g, 0) + int(m)\n\n    metrics: Dict[str, Any] = {\n        \"accuracy\": correct / max(total, 1),\n        \"loss\": loss_sum / max(total, 1),\n    }\n    if group_total:\n        group_acc = {str(k): group_correct[k] / group_total[k] for k in sorted(group_total)}\n        metrics[\"group_accuracy\"] = group_acc\n        metrics[\"worst_group_accuracy\"] = float(min(group_acc.values()))\n    return metrics"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Model: ResNet-18 (pretrained on ImageNet)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_model(num_classes: int = 2) -> nn.Module:\n    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n    return model\n\nmodel_pre = build_model(num_classes=2).to(DEVICE)\nprint(\"model ready\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Training with snapshot logging (fixed number of steps)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def save_checkpoint(path: Path, model: nn.Module, step: int, metrics: Optional[Dict[str, Any]] = None):\n    path.parent.mkdir(parents=True, exist_ok=True)\n    payload = {\n        \"step\": int(step),\n        \"state_dict\": {k: v.detach().cpu() for k, v in model.state_dict().items()},\n        \"metrics\": metrics or {},\n    }\n    torch.save(payload, path)\n\ndef load_checkpoint_state(path: Path) -> Dict[str, torch.Tensor]:\n    payload = torch.load(path, map_location=\"cpu\")\n    if \"state_dict\" in payload:\n        return payload[\"state_dict\"]\n    return payload\n\ndef train_with_snapshots(\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    test_loader: DataLoader,\n    device: torch.device,\n    max_steps: int,\n    lr: float,\n    weight_decay: float,\n    snapshot_every: int,\n    eval_every: int,\n    out_dir: Path,\n):\n    model = model.to(device)\n    opt = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, max_steps))\n    ce = nn.CrossEntropyLoss()\n\n    save_checkpoint(out_dir / \"pretrained.pt\", model, step=0, metrics={})\n\n    curve = []\n    step = 0\n    it = iter(train_loader)\n\n    while step < max_steps:\n        try:\n            x, ydict = next(it)\n        except StopIteration:\n            it = iter(train_loader)\n            x, ydict = next(it)\n\n        x = x.to(device)\n        y = ydict[\"y\"].to(device)\n\n        model.train()\n        logits = model(x)\n        loss = ce(logits, y)\n\n        opt.zero_grad(set_to_none=True)\n        loss.backward()\n        opt.step()\n        scheduler.step()\n        step += 1\n\n        if step % eval_every == 0 or step == max_steps:\n            val_metrics = evaluate_grouped(model, val_loader, device)\n            curve.append({\n                \"step\": step,\n                \"lr\": float(opt.param_groups[0][\"lr\"]),\n                \"train_loss\": float(loss.item()),\n                **val_metrics\n            })\n            print(f\"[step {step:4d}] lr={opt.param_groups[0]['lr']:.2e} \"\n                  f\"train_loss={loss.item():.4f} val_acc={val_metrics['accuracy']:.4f} \"\n                  f\"val_worst={val_metrics.get('worst_group_accuracy', float('nan')):.4f}\")\n\n        if step % snapshot_every == 0 or step == max_steps:\n            val_metrics = evaluate_grouped(model, val_loader, device)\n            save_checkpoint(out_dir / \"snapshots\" / f\"ckpt_{step:05d}.pt\", model, step=step, metrics=val_metrics)\n\n    test_metrics = evaluate_grouped(model, test_loader, device)\n    save_checkpoint(out_dir / \"final.pt\", model, step=step, metrics=test_metrics)\n\n    (out_dir / \"curve.json\").write_text(json.dumps(curve, indent=2))\n    return curve, test_metrics\n\ncurve, test_metrics = train_with_snapshots(\n    model_pre, train_loader, val_loader, test_loader, DEVICE,\n    max_steps=MAX_STEPS, lr=LR, weight_decay=WEIGHT_DECAY,\n    snapshot_every=SNAPSHOT_EVERY, eval_every=EVAL_EVERY,\n    out_dir=LOG_DIR\n)\n\nprint(\"Final test metrics:\", test_metrics)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Task vector + random-like baseline"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def compute_task_vector(pre_state: Dict[str, torch.Tensor], ft_state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n    vec = {}\n    for k in pre_state.keys():\n        vec[k] = (ft_state[k] - pre_state[k]).cpu()\n    return vec\n\ndef vector_stats(vec: Dict[str, torch.Tensor]) -> Dict[str, Any]:\n    stats = {}\n    total_sq = 0.0\n    for k, v in vec.items():\n        n = float(v.float().norm().item())\n        stats[k] = {\"shape\": list(v.shape), \"l2_norm\": n}\n        total_sq += n**2\n    stats[\"_total_l2_norm\"] = float(math.sqrt(total_sq))\n    return stats\n\ndef random_like_vector(task_vec: Dict[str, torch.Tensor], seed: int = 0) -> Dict[str, torch.Tensor]:\n    g = torch.Generator()\n    g.manual_seed(seed)\n    out = {}\n    for k, v in task_vec.items():\n        r = torch.randn(v.shape, generator=g)\n        v_norm = v.float().norm()\n        r_norm = r.float().norm() + 1e-12\n        out[k] = (r * (v_norm / r_norm)).cpu()\n    return out\n\npre_state = load_checkpoint_state(LOG_DIR / \"pretrained.pt\")\nft_state  = load_checkpoint_state(LOG_DIR / \"final.pt\")\n\ntask_vec = compute_task_vector(pre_state, ft_state)\nrand_vec = random_like_vector(task_vec, seed=SEED)\n\ntorch.save(task_vec, RES_DIR / \"task_vector.pt\")\ntorch.save(rand_vec, RES_DIR / \"random_like_vector.pt\")\n(RES_DIR / \"task_vector_stats.json\").write_text(json.dumps(vector_stats(task_vec), indent=2))\n\nprint(\"Saved task vectors to:\", RES_DIR)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Apply edits and evaluate (pretrained + alpha * v_T)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def apply_task_edit(pre_state: Dict[str, torch.Tensor], vec: Dict[str, torch.Tensor], alpha: float) -> Dict[str, torch.Tensor]:\n    return {k: (pre_state[k].cpu() + alpha * vec[k].cpu()) for k in pre_state.keys()}\n\ndef save_state_as_ckpt(path: Path, state: Dict[str, torch.Tensor], step: int = 0, metrics: Optional[Dict[str, Any]] = None):\n    payload = {\"step\": int(step), \"state_dict\": state, \"metrics\": metrics or {}}\n    torch.save(payload, path)\n\ndef eval_state_dict(state: Dict[str, torch.Tensor]) -> Dict[str, Any]:\n    m = build_model(num_classes=2).to(DEVICE)\n    m.load_state_dict(state, strict=True)\n    return evaluate_grouped(m, test_loader, DEVICE)\n\ndef eval_ckpt(path: Path) -> Dict[str, Any]:\n    st = load_checkpoint_state(path)\n    return eval_state_dict(st)\n\n# Make edited checkpoints from pretrained weights\nforget_state = apply_task_edit(pre_state, task_vec, alpha=-1.0)\nhalf_state   = apply_task_edit(pre_state, task_vec, alpha=0.5)\nadd_state    = apply_task_edit(pre_state, task_vec, alpha=1.0)\nrand_state   = apply_task_edit(pre_state, rand_vec, alpha=1.0)\n\nsave_state_as_ckpt(RES_DIR / \"forget.pt\", forget_state)\nsave_state_as_ckpt(RES_DIR / \"half.pt\", half_state)\nsave_state_as_ckpt(RES_DIR / \"add.pt\", add_state)\nsave_state_as_ckpt(RES_DIR / \"random_baseline.pt\", rand_state)\n\nckpts = {\n    \"pretrained\": LOG_DIR / \"pretrained.pt\",\n    \"finetuned\": LOG_DIR / \"final.pt\",\n    \"forget\": RES_DIR / \"forget.pt\",\n    \"half\": RES_DIR / \"half.pt\",\n    \"add\": RES_DIR / \"add.pt\",\n    \"random\": RES_DIR / \"random_baseline.pt\",\n}\n\nedited_metrics = {name: eval_ckpt(path) for name, path in ckpts.items()}\n\nfor name, met in edited_metrics.items():\n    print(name, \"acc=\", met[\"accuracy\"], \"worst=\", met.get(\"worst_group_accuracy\", None))\n\n(RES_DIR / \"edited_metrics.json\").write_text(json.dumps(edited_metrics, indent=2))\nwith (RES_DIR / \"edited_metrics.csv\").open(\"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\"name\", \"accuracy\", \"loss\", \"worst_group_accuracy\"])\n    for name, met in edited_metrics.items():\n        w.writerow([name, met.get(\"accuracy\"), met.get(\"loss\"), met.get(\"worst_group_accuracy\")])\n\nprint(\"Wrote metrics to:\", RES_DIR)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Trajectory analysis: PCA + alignment"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def flatten_state(state: Dict[str, torch.Tensor]) -> np.ndarray:\n    parts = [v.detach().cpu().float().reshape(-1).numpy() for v in state.values()]\n    return np.concatenate(parts) if parts else np.zeros((0,), dtype=np.float32)\n\nsnapshots = sorted(SNAP_DIR.glob(\"ckpt_*.pt\"))\nsteps = []\ndelta_mat = []\nalignments = []\n\nflat_task = flatten_state(task_vec)\nflat_task_norm = np.linalg.norm(flat_task) + 1e-12\n\nfor p in snapshots:\n    m = re.search(r\"ckpt_(\\d+)\\.pt$\", p.name)\n    if not m:\n        continue\n    step = int(m.group(1))\n    steps.append(step)\n\n    st = load_checkpoint_state(p)\n    delta = {k: st[k] - pre_state[k] for k in st.keys()}\n    flat_delta = flatten_state(delta)\n    delta_mat.append(flat_delta)\n\n    num = float(np.dot(flat_delta, flat_task))\n    den = float((np.linalg.norm(flat_delta) + 1e-12) * flat_task_norm)\n    alignments.append(num / den)\n\nsteps = np.array(steps, dtype=np.int64)\ndelta_mat = np.stack(delta_mat, axis=0)\n\npca = PCA(n_components=2)\nxy = pca.fit_transform(delta_mat)\nexpl = pca.explained_variance_ratio_.tolist()\n\n(RES_DIR / \"pca_summary.json\").write_text(json.dumps({\"explained_variance_ratio\": expl}, indent=2))\nprint(\"PCA explained variance ratio:\", expl)\n\n# PCA plot\nplt.figure(figsize=(6,5))\nplt.scatter(xy[:,0], xy[:,1], c=steps, s=40)\nfor i, s in enumerate(steps):\n    plt.text(xy[i,0], xy[i,1], str(s), fontsize=8)\nplt.title(\"Weight trajectory PCA (deltas from pretrained)\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.colorbar(label=\"step\")\npca_path = RES_DIR / \"trajectory_pca.png\"\nplt.tight_layout(); plt.savefig(pca_path, dpi=150); plt.show()\n\n# Alignment plot\nplt.figure(figsize=(6,4))\nplt.plot(steps, alignments, marker=\"o\")\nplt.title(\"Cosine alignment: delta_w(t) vs final task vector\")\nplt.xlabel(\"step\"); plt.ylabel(\"cosine similarity\")\nalign_path = RES_DIR / \"alignment_curve.png\"\nplt.tight_layout(); plt.savefig(align_path, dpi=150); plt.show()\n\nprint(\"Saved:\", pca_path, \"and\", align_path)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10) Extra plots (learning curves, edited comparison, heatmap, alpha sweep)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "curve = json.loads((LOG_DIR / \"curve.json\").read_text())\ncurve_steps = np.array([c[\"step\"] for c in curve], dtype=int)\nval_acc = np.array([c[\"accuracy\"] for c in curve], dtype=float)\nval_worst = np.array([c.get(\"worst_group_accuracy\", np.nan) for c in curve], dtype=float)\n\nplt.figure(figsize=(6,4))\nplt.plot(curve_steps, val_acc, marker=\"o\", label=\"val accuracy\")\nplt.plot(curve_steps, val_worst, marker=\"o\", label=\"val worst-group\")\nplt.title(\"Validation learning curves\")\nplt.xlabel(\"step\"); plt.ylabel(\"metric\")\nplt.legend()\nlc_path = RES_DIR / \"learning_curves.png\"\nplt.tight_layout(); plt.savefig(lc_path, dpi=150); plt.show()\n\nnames = list(edited_metrics.keys())\noverall = [edited_metrics[n][\"accuracy\"] for n in names]\nworst = [edited_metrics[n].get(\"worst_group_accuracy\", np.nan) for n in names]\nx = np.arange(len(names))\nw = 0.38\n\nplt.figure(figsize=(8,4))\nplt.bar(x - w/2, overall, w, label=\"overall acc\")\nplt.bar(x + w/2, worst, w, label=\"worst-group acc\")\nplt.xticks(x, names, rotation=30, ha=\"right\")\nplt.title(\"Edited models: overall vs worst-group (test)\")\nplt.ylabel(\"accuracy\")\nplt.legend()\nbar_path = RES_DIR / \"edited_models_bar.png\"\nplt.tight_layout(); plt.savefig(bar_path, dpi=150); plt.show()\n\ndef group_vec(met: Dict[str, Any], num_groups: int = 4) -> np.ndarray:\n    g = met.get(\"group_accuracy\", {})\n    out = np.zeros((num_groups,), dtype=float)\n    for i in range(num_groups):\n        out[i] = float(g.get(str(i), np.nan))\n    return out\n\nftg = group_vec(edited_metrics[\"finetuned\"], 4)\nfgg = group_vec(edited_metrics[\"forget\"], 4)\nmat = np.vstack([ftg, fgg])\n\nplt.figure(figsize=(7,2.5))\nplt.imshow(mat, aspect=\"auto\")\nplt.yticks([0,1], [\"finetuned\", \"forget(-1)\"])\nplt.xticks([0,1,2,3], [\"y0-bg0\",\"y0-bg1\",\"y1-bg0\",\"y1-bg1\"])\nplt.colorbar(label=\"accuracy\")\nplt.title(\"Per-group accuracy (test)\")\nfor i in range(mat.shape[0]):\n    for j in range(mat.shape[1]):\n        plt.text(j, i, f\"{mat[i,j]:.2f}\", ha=\"center\", va=\"center\", fontsize=10)\nheat_path = RES_DIR / \"group_accuracy_heatmap.png\"\nplt.tight_layout(); plt.savefig(heat_path, dpi=150); plt.show()\n\nalphas = [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]\nsweep = []\nfor a in alphas:\n    st = apply_task_edit(pre_state, task_vec, alpha=a)\n    met = eval_state_dict(st)\n    sweep.append({\"alpha\": a, **met})\n\n(RES_DIR / \"alpha_sweep.json\").write_text(json.dumps(sweep, indent=2))\naccs = [s[\"accuracy\"] for s in sweep]\nworsts = [s.get(\"worst_group_accuracy\", np.nan) for s in sweep]\n\nplt.figure(figsize=(6,4))\nplt.plot(alphas, accs, marker=\"o\", label=\"overall acc\")\nplt.plot(alphas, worsts, marker=\"o\", label=\"worst-group acc\")\nplt.axvline(0.0, linestyle=\"--\")\nplt.title(\"Alpha sweep: pretrained + alpha * v_T\")\nplt.xlabel(\"alpha\"); plt.ylabel(\"accuracy\")\nplt.legend()\nsweep_path = RES_DIR / \"alpha_sweep.png\"\nplt.tight_layout(); plt.savefig(sweep_path, dpi=150); plt.show()\n\nprint(\"Saved plots to:\", RES_DIR)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11) Outputs\n\nAll artifacts are saved under:\n\n- `outputs/waterbirds/logs/`  \n  - `pretrained.pt`, `final.pt`, `curve.json`, and snapshots in `snapshots/`\n\n- `outputs/waterbirds/results/`  \n  - task vectors: `task_vector.pt`, `random_like_vector.pt`  \n  - edited checkpoints: `forget.pt`, `half.pt`, `add.pt`, `random_baseline.pt`  \n  - metrics: `edited_metrics.json`, `edited_metrics.csv`  \n  - analysis: `pca_summary.json`, `alpha_sweep.json`  \n  - figures: `trajectory_pca.png`, `alignment_curve.png`, `learning_curves.png`, `edited_models_bar.png`, `group_accuracy_heatmap.png`, `alpha_sweep.png`\n\nOn Kaggle you can download them from the **Output** panel."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}